{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "﻿History of web search engines\n"
     ]
    }
   ],
   "source": [
    "with open(\"History of web search engines.txt\") as file:\n",
    "    text = file.read()\n",
    "    print(type(text))\n",
    "    print(text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "﻿History of web search engines-:\n"
     ]
    }
   ],
   "source": [
    "#alternatively\n",
    "f = open(\"History of web search engines.txt\")\n",
    "text = f.read()\n",
    "print(type(text))\n",
    "print(text[:32])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# electronic books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "﻿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "text = response.read().decode(\"utf-8\")\n",
    "print(type(text))\n",
    "print(text[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pdf file object\n",
    "pdfFileObject = open(\"the-road-to-learn-react.pdf\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "204\n"
     ]
    }
   ],
   "source": [
    "#create pdf reader object\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    "print(pdfReader.isEncrypted) #to check if pdf is encrypted\n",
    "print(pdfReader.numPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "#get page object\n",
    "pageObj = pdfReader.getPage(66)\n",
    "text = pageObj.extractText()\n",
    "print(type(text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close pdf object\n",
    "pdfFileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### above method doesn't work on all pdfs so there is an alternative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.call(['pdftotext','History of web search engines.pdf','output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "History of web search engines-\n"
     ]
    }
   ],
   "source": [
    "f = open(\"output\")\n",
    "text = f.read()\n",
    "print(type(text))\n",
    "print(text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### another alternative library to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "<class 'str'>\n",
      "History of web search engines-\n",
      "<class 'str'>\n",
      "The ElNet Galaxy​ was a web di\n",
      "page 1 History of web search engines-\n",
      "page 2  The ElNet Galaxy​ was a web di\n",
      "3874\n",
      "3743\n",
      "7619\n"
     ]
    }
   ],
   "source": [
    "with open(\"History of web search engines.pdf\",\"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f)      #if password protected then pass password as second argument\n",
    "print(len(pdf)) #no of pages\n",
    "for page in pdf:\n",
    "    print(type(page))\n",
    "    print(page[:30])\n",
    "print(\"page 1\",pdf[0][:30])\n",
    "print(\"page 2 \",pdf[1][:30])\n",
    "print(len(pdf[0]))\n",
    "print(len(pdf[1]))\n",
    "print(len(\"\\n\\n\".join(pdf)))      #read all text in one string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docx.Document(\"History of web search engines.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(doc.paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History of web search engines-:\n"
     ]
    }
   ],
   "source": [
    "print(doc.paragraphs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(doc.paragraphs[2].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(doc.paragraphs[2].runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The need for search engine first surfaced in July 1945. Vannevar Bush published an article \"As We May Think\" in Atlantic monthly. Few lines from his article were -: \"The difficulty seems to be, not so much that we publish unduly in view of the extent and variety of present-day interests, but rather that publication has been extended far beyond our present ability to make real use of the record\" which shed light on a greater issue i.e. we need an index to be built on the data we have as the data is growing so much so we are unable to make real use of it. So he proposed the idea of a system called Memex. Memex is a blend of words memory and index and is essentially a device in which people would compress and store their books, communications and records, it is done in such a manner that it will allow easy, fast and flexible access to content in it and this idea of memex influenced the development of early hypertext systems and eventually lead to creation of WWW(world wide web).\n"
     ]
    }
   ],
   "source": [
    "print((doc.paragraphs[2].runs[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to get full text out of docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7588\n"
     ]
    }
   ],
   "source": [
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return \"\\n\".join(fullText)\n",
    "print(len(getText(\"History of web search engines.docx\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
